{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "wrP5ioM3IeZh",
    "outputId": "b68ca14a-3386-481e-9355-f05c29ca8338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCXAgnaLHrZ5"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('/content/drive/My Drive/Colab_Notebooks/DeepLearningProjects/KerasWordLevelGeneration/JasonBrownlee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "IDF-vDxZLeDX",
    "outputId": "ae8d93d8-6bdf-48dc-9251-8cff5a4012e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Dense, Input, LSTM\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LJsKEHe_Iwc8",
    "outputId": "93845cb7-5764-4aae-cad6-3fa84099d0c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15802\n"
     ]
    }
   ],
   "source": [
    "with open('republic.txt', mode='r') as file:\n",
    "    print(len(file.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UX75yePJLkJ"
   },
   "source": [
    "### Data Prepration\n",
    "\n",
    "\n",
    "* Book/Chapter headings (e.g. “BOOK I.”).\n",
    "* British English spelling (e.g. “honoured”)\n",
    "* Lots of punctuation (e.g. “–“, “;–“, “?–“, and more)\n",
    "* Strange names (e.g. “Polemarchus”).\n",
    "* Some long monologues that go on for hundreds of lines.\n",
    "* Some quoted dialog (e.g. ‘…’)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "GfzqsKaiI9QZ",
    "outputId": "c3a999eb-25b5-4a6b-9256-781af627b8e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK I.\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in what\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load document\n",
    "in_filename = 'republic.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qqjrw1qLDlg"
   },
   "source": [
    "### Clean Text\n",
    "\n",
    "\n",
    "* Replace ‘–‘ with a white space so we can split words better.\n",
    "* Split words based on white space.\n",
    "* Remove all punctuation from words to reduce the vocabulary size (e.g. ‘What?’ becomes ‘What’).\n",
    "* Remove all words that are not alphabetic to remove standalone punctuation tokens.\n",
    "* Normalize all words to lowercase to reduce the vocabulary size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3p5NdGXdLArI"
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "hpn1egY2Mtmt",
    "outputId": "e48d49e1-ed48-476d-9408-6c73f6739b9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n",
      "Total Tokens: 118684\n",
      "Unique Tokens: 7409\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8t3bZjhlMuGt",
    "outputId": "191b8b61-a48f-42fb-b9ce-9eaeb6a178a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 118633\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DKNyMPfyNdsW"
   },
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZzUiw8yAN6q0"
   },
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F88pTHfiOFv9"
   },
   "source": [
    "# Train Language Model\n",
    "\n",
    "* It uses a distributed representation for words so that different words with similar meanings will have a similar representation.\n",
    "* It learns the representation at the same time as learning the model.\n",
    "* It learns to predict the probability for the next word using the context of the last 100 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqDUzo5fN-Iu"
   },
   "outputs": [],
   "source": [
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2Zf_8HrO7P6"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "mCLffaAtl8Kf",
    "outputId": "ae60483d-e792-4745-b1ad-910286b195bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is:  7410\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('vocab size is: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "hH8Z5eSznMID",
    "outputId": "3d6d7995-f1b3-41b8-a178-fb2444fc18c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence length is:  50\n",
      "x_train shape (118633, 50)\n",
      "y_train shape (118633, 7410)\n"
     ]
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "x_train, y_train = sequences[:, :-1], sequences[:, -1]\n",
    "y_train = to_categorical(y_train, num_classes = vocab_size)\n",
    "seq_length = x_train.shape[1]\n",
    "print('sequence length is: ', seq_length)\n",
    "print('x_train shape', x_train.shape)\n",
    "print('y_train shape', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "colab_type": "code",
    "id": "J-2Mp8_LoamC",
    "outputId": "dce84f1d-abe0-4d56-e92e-8078b2ae5cf0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0902 09:59:37.514068  5444 deprecation_wrapper.py:119] From C:\\Users\\10\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0902 09:59:37.529200  5444 deprecation_wrapper.py:119] From C:\\Users\\10\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0902 09:59:37.530328  5444 deprecation_wrapper.py:119] From C:\\Users\\10\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0902 09:59:37.931329  5444 deprecation_wrapper.py:119] From C:\\Users\\10\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0902 09:59:37.954182  5444 deprecation_wrapper.py:119] From C:\\Users\\10\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 50)            370500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 128)           91648     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7410)              955890    \n",
      "=================================================================\n",
      "Total params: 1,566,134\n",
      "Trainable params: 1,566,134\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "inputs = Input(shape=(50,) ,name='inputs')\n",
    "x = inputs\n",
    "x = Embedding(input_dim= vocab_size, output_dim=50)(inputs)\n",
    "x = LSTM(128, return_sequences= True)(x)\n",
    "x= LSTM(128)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "outputs = Dense(vocab_size, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "colab_type": "code",
    "id": "5ZkeXFZ3q1NE",
    "outputId": "de5a072b-9efa-4552-977d-604386864489"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0831 22:26:33.004870  2896 deprecation_wrapper.py:119] From C:\\Users\\10\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0831 22:26:33.100643  2896 deprecation_wrapper.py:119] From C:\\Users\\10\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0831 22:26:33.208351  2896 deprecation.py:323] From C:\\Users\\10\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0831 22:26:34.516002  2896 deprecation_wrapper.py:119] From C:\\Users\\10\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "118633/118633 [==============================] - 145s 1ms/step - loss: 6.1330 - acc: 0.0741\n",
      "Epoch 2/200\n",
      "118633/118633 [==============================] - 142s 1ms/step - loss: 5.6987 - acc: 0.1070\n",
      "Epoch 3/200\n",
      "118633/118633 [==============================] - 140s 1ms/step - loss: 5.4531 - acc: 0.1353\n",
      "Epoch 4/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 5.2907 - acc: 0.1489\n",
      "Epoch 5/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 5.2162 - acc: 0.1532\n",
      "Epoch 6/200\n",
      "118633/118633 [==============================] - 135s 1ms/step - loss: 5.0960 - acc: 0.1619\n",
      "Epoch 7/200\n",
      "118633/118633 [==============================] - 135s 1ms/step - loss: 4.9977 - acc: 0.1673\n",
      "Epoch 8/200\n",
      "118633/118633 [==============================] - 135s 1ms/step - loss: 4.9085 - acc: 0.1728\n",
      "Epoch 9/200\n",
      "118633/118633 [==============================] - 137s 1ms/step - loss: 4.8246 - acc: 0.1765\n",
      "Epoch 10/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 4.7458 - acc: 0.1804\n",
      "Epoch 11/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 4.6706 - acc: 0.1842\n",
      "Epoch 12/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 4.5971 - acc: 0.1875\n",
      "Epoch 13/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 4.5242 - acc: 0.1913\n",
      "Epoch 14/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 4.4566 - acc: 0.1942\n",
      "Epoch 15/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 4.3915 - acc: 0.1973\n",
      "Epoch 16/200\n",
      "118633/118633 [==============================] - 137s 1ms/step - loss: 4.3283 - acc: 0.1999\n",
      "Epoch 17/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 4.2803 - acc: 0.2016\n",
      "Epoch 18/200\n",
      "118633/118633 [==============================] - 135s 1ms/step - loss: 4.2907 - acc: 0.2010\n",
      "Epoch 19/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 4.2731 - acc: 0.2008\n",
      "Epoch 20/200\n",
      "118633/118633 [==============================] - 135s 1ms/step - loss: 4.2042 - acc: 0.2050\n",
      "Epoch 21/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 4.1587 - acc: 0.2077\n",
      "Epoch 22/200\n",
      "118633/118633 [==============================] - 137s 1ms/step - loss: 4.1035 - acc: 0.2110\n",
      "Epoch 23/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 4.0336 - acc: 0.2168\n",
      "Epoch 24/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 3.9874 - acc: 0.2194\n",
      "Epoch 25/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 3.9528 - acc: 0.2222\n",
      "Epoch 26/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 3.9105 - acc: 0.2267\n",
      "Epoch 27/200\n",
      "118633/118633 [==============================] - 136s 1ms/step - loss: 3.8614 - acc: 0.2313\n",
      "Epoch 28/200\n",
      "118633/118633 [==============================] - 142s 1ms/step - loss: 3.8083 - acc: 0.2376\n",
      "Epoch 29/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.7650 - acc: 0.2409\n",
      "Epoch 30/200\n",
      "118633/118633 [==============================] - 134s 1ms/step - loss: 3.7234 - acc: 0.2441\n",
      "Epoch 31/200\n",
      "118633/118633 [==============================] - 134s 1ms/step - loss: 3.6734 - acc: 0.2503\n",
      "Epoch 32/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.6325 - acc: 0.2543\n",
      "Epoch 33/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.5885 - acc: 0.2598\n",
      "Epoch 34/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.5498 - acc: 0.2636\n",
      "Epoch 35/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.5130 - acc: 0.2692\n",
      "Epoch 36/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.4743 - acc: 0.2720\n",
      "Epoch 37/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.4388 - acc: 0.2775\n",
      "Epoch 38/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.4095 - acc: 0.2810\n",
      "Epoch 39/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.3731 - acc: 0.2862\n",
      "Epoch 40/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.3606 - acc: 0.2894\n",
      "Epoch 41/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.4011 - acc: 0.2873\n",
      "Epoch 42/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.4680 - acc: 0.2815\n",
      "Epoch 43/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.4006 - acc: 0.2877\n",
      "Epoch 44/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.3596 - acc: 0.2942\n",
      "Epoch 45/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.3368 - acc: 0.2967\n",
      "Epoch 46/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.3151 - acc: 0.2997\n",
      "Epoch 47/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.2554 - acc: 0.3074\n",
      "Epoch 48/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.2029 - acc: 0.3137\n",
      "Epoch 49/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.1713 - acc: 0.3180\n",
      "Epoch 50/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.1422 - acc: 0.3229\n",
      "Epoch 51/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.1367 - acc: 0.3251\n",
      "Epoch 52/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.1274 - acc: 0.3269\n",
      "Epoch 53/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.1266 - acc: 0.3291\n",
      "Epoch 54/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.1695 - acc: 0.3267\n",
      "Epoch 55/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.1890 - acc: 0.3253\n",
      "Epoch 56/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.4568 - acc: 0.2951\n",
      "Epoch 57/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.3347 - acc: 0.3086\n",
      "Epoch 58/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 3.2504 - acc: 0.3175\n",
      "Epoch 59/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.2613 - acc: 0.3161\n",
      "Epoch 60/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.2005 - acc: 0.3239\n",
      "Epoch 61/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 3.0279 - acc: 0.3425\n",
      "Epoch 62/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.9661 - acc: 0.3524\n",
      "Epoch 63/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.9065 - acc: 0.3614\n",
      "Epoch 64/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 2.8746 - acc: 0.3667\n",
      "Epoch 65/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 2.8407 - acc: 0.3728\n",
      "Epoch 66/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.8186 - acc: 0.3775\n",
      "Epoch 67/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.7818 - acc: 0.3836\n",
      "Epoch 68/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.7519 - acc: 0.3880\n",
      "Epoch 69/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.7443 - acc: 0.3896\n",
      "Epoch 70/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.6924 - acc: 0.3983\n",
      "Epoch 71/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.6536 - acc: 0.4024\n",
      "Epoch 72/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 2.6168 - acc: 0.4093\n",
      "Epoch 73/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.5855 - acc: 0.4149\n",
      "Epoch 74/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.5727 - acc: 0.4179\n",
      "Epoch 75/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.5439 - acc: 0.4230\n",
      "Epoch 76/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.5353 - acc: 0.4269\n",
      "Epoch 77/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.4964 - acc: 0.4317\n",
      "Epoch 78/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 2.4718 - acc: 0.4356\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.4499 - acc: 0.4402\n",
      "Epoch 80/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.4655 - acc: 0.4392\n",
      "Epoch 81/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.4163 - acc: 0.4489\n",
      "Epoch 82/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.3931 - acc: 0.4539\n",
      "Epoch 83/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.3616 - acc: 0.4574\n",
      "Epoch 84/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.3475 - acc: 0.4626\n",
      "Epoch 85/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 2.3249 - acc: 0.4669\n",
      "Epoch 86/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.3350 - acc: 0.4671\n",
      "Epoch 87/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.2765 - acc: 0.4762\n",
      "Epoch 88/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.2501 - acc: 0.4804\n",
      "Epoch 89/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.2566 - acc: 0.4814\n",
      "Epoch 90/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.2265 - acc: 0.4873\n",
      "Epoch 91/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.2051 - acc: 0.4920\n",
      "Epoch 92/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 2.2190 - acc: 0.4933\n",
      "Epoch 93/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.1796 - acc: 0.5001\n",
      "Epoch 94/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.1858 - acc: 0.4998\n",
      "Epoch 95/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.1642 - acc: 0.5015\n",
      "Epoch 96/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.1830 - acc: 0.4970\n",
      "Epoch 97/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.1144 - acc: 0.5127\n",
      "Epoch 98/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.0675 - acc: 0.5226\n",
      "Epoch 99/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 2.0313 - acc: 0.5278\n",
      "Epoch 100/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 2.0094 - acc: 0.5316\n",
      "Epoch 101/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.0096 - acc: 0.5313\n",
      "Epoch 102/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.9972 - acc: 0.5346\n",
      "Epoch 103/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.9681 - acc: 0.5396\n",
      "Epoch 104/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.9842 - acc: 0.5377\n",
      "Epoch 105/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.9497 - acc: 0.5463\n",
      "Epoch 106/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 2.1640 - acc: 0.5217\n",
      "Epoch 107/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.1826 - acc: 0.5234\n",
      "Epoch 108/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.1104 - acc: 0.5290\n",
      "Epoch 109/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.3158 - acc: 0.5069\n",
      "Epoch 110/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.1632 - acc: 0.5288\n",
      "Epoch 111/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.9432 - acc: 0.5450\n",
      "Epoch 112/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.8970 - acc: 0.5581\n",
      "Epoch 113/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.8500 - acc: 0.5652\n",
      "Epoch 114/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.8142 - acc: 0.5744\n",
      "Epoch 115/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.8097 - acc: 0.5747\n",
      "Epoch 116/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.7988 - acc: 0.5756\n",
      "Epoch 117/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.8331 - acc: 0.5749\n",
      "Epoch 118/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 2.0713 - acc: 0.5478\n",
      "Epoch 119/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.9689 - acc: 0.5624\n",
      "Epoch 120/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.9836 - acc: 0.5609\n",
      "Epoch 121/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.9634 - acc: 0.5665\n",
      "Epoch 122/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.9082 - acc: 0.5697\n",
      "Epoch 123/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.8533 - acc: 0.5792\n",
      "Epoch 124/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.8379 - acc: 0.5810\n",
      "Epoch 125/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.8134 - acc: 0.5883\n",
      "Epoch 126/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.8276 - acc: 0.5853\n",
      "Epoch 127/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.7694 - acc: 0.5958\n",
      "Epoch 128/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.7539 - acc: 0.5984\n",
      "Epoch 129/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.7465 - acc: 0.5988\n",
      "Epoch 130/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.7322 - acc: 0.6038\n",
      "Epoch 131/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.7041 - acc: 0.6093\n",
      "Epoch 132/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.7705 - acc: 0.5992\n",
      "Epoch 133/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.7987 - acc: 0.6003\n",
      "Epoch 134/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.7171 - acc: 0.6133\n",
      "Epoch 135/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.7583 - acc: 0.6050\n",
      "Epoch 136/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.6649 - acc: 0.6244\n",
      "Epoch 137/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.6413 - acc: 0.6273\n",
      "Epoch 138/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.6276 - acc: 0.6283\n",
      "Epoch 139/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.7793 - acc: 0.6028\n",
      "Epoch 140/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.7190 - acc: 0.6218\n",
      "Epoch 141/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.6516 - acc: 0.6287\n",
      "Epoch 142/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.5977 - acc: 0.6340\n",
      "Epoch 143/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.5619 - acc: 0.6429\n",
      "Epoch 144/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.5442 - acc: 0.6465\n",
      "Epoch 145/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.5418 - acc: 0.6475\n",
      "Epoch 146/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.5586 - acc: 0.6453\n",
      "Epoch 147/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.5421 - acc: 0.6465\n",
      "Epoch 148/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.5203 - acc: 0.6542\n",
      "Epoch 149/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.5808 - acc: 0.6389\n",
      "Epoch 150/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.4933 - acc: 0.6573\n",
      "Epoch 151/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.4635 - acc: 0.6650\n",
      "Epoch 152/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.4621 - acc: 0.6631\n",
      "Epoch 153/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.4338 - acc: 0.6688\n",
      "Epoch 154/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.4282 - acc: 0.6694\n",
      "Epoch 155/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.4467 - acc: 0.6650\n",
      "Epoch 156/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.4511 - acc: 0.6616\n",
      "Epoch 157/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.4203 - acc: 0.6708\n",
      "Epoch 158/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.4002 - acc: 0.6786\n",
      "Epoch 159/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.3882 - acc: 0.6794\n",
      "Epoch 160/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.3571 - acc: 0.6866\n",
      "Epoch 161/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.3521 - acc: 0.6880\n",
      "Epoch 162/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.3552 - acc: 0.6866\n",
      "Epoch 163/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.3303 - acc: 0.6944\n",
      "Epoch 164/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.3298 - acc: 0.6939\n",
      "Epoch 165/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.3862 - acc: 0.6751\n",
      "Epoch 166/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2769 - acc: 0.7071\n",
      "Epoch 167/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.2585 - acc: 0.7112\n",
      "Epoch 168/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2536 - acc: 0.7104\n",
      "Epoch 169/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2650 - acc: 0.7087\n",
      "Epoch 170/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2612 - acc: 0.7075\n",
      "Epoch 171/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2437 - acc: 0.7129\n",
      "Epoch 172/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.3225 - acc: 0.6910\n",
      "Epoch 173/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2157 - acc: 0.7214\n",
      "Epoch 174/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.2027 - acc: 0.7237\n",
      "Epoch 175/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2067 - acc: 0.7221\n",
      "Epoch 176/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2167 - acc: 0.7211\n",
      "Epoch 177/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2455 - acc: 0.7118\n",
      "Epoch 178/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.1719 - acc: 0.7294\n",
      "Epoch 179/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2863 - acc: 0.7037\n",
      "Epoch 180/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2089 - acc: 0.7263\n",
      "Epoch 181/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.1662 - acc: 0.7347\n",
      "Epoch 182/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2577 - acc: 0.7064\n",
      "Epoch 183/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.1688 - acc: 0.7298\n",
      "Epoch 184/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.1153 - acc: 0.7454\n",
      "Epoch 185/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.1108 - acc: 0.7456\n",
      "Epoch 186/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.1439 - acc: 0.7354\n",
      "Epoch 187/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.1372 - acc: 0.7384\n",
      "Epoch 188/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.1378 - acc: 0.7392\n",
      "Epoch 189/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.1155 - acc: 0.7436\n",
      "Epoch 190/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.1910 - acc: 0.7285\n",
      "Epoch 191/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.1039 - acc: 0.7475\n",
      "Epoch 192/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2531 - acc: 0.7226\n",
      "Epoch 193/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2247 - acc: 0.7297\n",
      "Epoch 194/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.1662 - acc: 0.7416\n",
      "Epoch 195/200\n",
      "118633/118633 [==============================] - 133s 1ms/step - loss: 1.1819 - acc: 0.7351\n",
      "Epoch 196/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.1146 - acc: 0.7486\n",
      "Epoch 197/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.0836 - acc: 0.7563\n",
      "Epoch 198/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.0871 - acc: 0.7536\n",
      "Epoch 199/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.2350 - acc: 0.7243\n",
      "Epoch 200/200\n",
      "118633/118633 [==============================] - 132s 1ms/step - loss: 1.3144 - acc: 0.7164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cf9e409da0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=128, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "q898wtf8sfkg",
    "outputId": "b84ac279-f3c2-45c3-8b98-93b95fc3c5fd"
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDUz3h8sslBe"
   },
   "source": [
    "# Use Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here are three beds one existing in nature which is made by god as i think that we may say for no one else can be the maker no there is another which is the work of the carpenter yes and the work of the painter is a third yes beds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[np.random.randint(0, len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 51)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = np.array(tokenizer.texts_to_sequences([seed_text]))\n",
    "# predict the probabilities for each word\n",
    "y_hat = np.argmax(model.predict(encoded[:,:seq_length], verbose=0)[0])\n",
    "out_word = ''\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index == y_hat:\n",
    "        out_word = word\n",
    "        break\n",
    "print('predicted word is: ', out_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = np.array(tokenizer.texts_to_sequences([in_text]))\n",
    "        encoded = pad_sequences(encoded, maxlen=seq_length, truncating='pre')\n",
    "        y_hat = np.argmax(model.predict(encoded, verbose=0)[0])\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == y_hat:\n",
    "                out_word = word\n",
    "                break\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here are three beds one existing in nature which is made by god as i think that we may say for no one else can be the maker no there is another which is the work of the carpenter yes and the work of the painter is a third yes beds\n",
      "then are of man as in what men is not as a man to hold in the first place that he has no cares appetites and then they are prescribing because the lovers of natures have a faculty of poets he is the only reason yes he replied then has\n"
     ]
    }
   ],
   "source": [
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(seed_text)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pretrained Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*:)*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word_level.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
